# QuantTrader - Reinforcement Learning Trading Agent

QuantTrader is a deep reinforcement learning agent that autonomously learns to trade stocks for maximum profit using historical and real-time market data. It leverages custom Gym environments and RLlib (Ray) to train, simulate, and optionally paper trade in real-time.

## ğŸŒ Features
- PPO-based stock trading agent
- Alpha Vantage integration for **historical and real-time data**
- Real-time data streaming support for paper/live trading
- Custom OpenAI Gym trading environment
- Reward shaping with transaction cost and portfolio tracking
- Modular codebase for training, inference, and evaluation
- Jupyter notebooks for EDA and visualization
- Config-driven setup for reproducibility

## ğŸ›  Tech Stack
- Python, PyTorch
- Ray + RLlib
- OpenAI Gym
- Alpha Vantage API
- Pandas, NumPy, Matplotlib
- YAML for config management
- Dotenv for API key security

## ğŸ“ Project Structure
```
QuantTrader/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ env.yaml              # Trading environment parameters (window size, cost)
â”‚   â”œâ”€â”€ train.yaml            # RL training hyperparameters
â”‚   â””â”€â”€ logging.yaml          # Logging and output settings
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ historical/           # Pre-downloaded CSV data (for training)
â”‚   â””â”€â”€ raw/                  # Raw downloaded data files (if any)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ analysis.ipynb        # EDA and feature exploration
â”‚   â””â”€â”€ results_visualization.ipynb   # Visuals of training metrics
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ env/
â”‚   â”‚   â”œâ”€â”€ stock_trading_env.py   # Custom Gym-compatible environment
â”‚   â”‚   â””â”€â”€ utils.py               # Performance metrics and helpers
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_pipeline.py       # Historical + real-time data loading
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â””â”€â”€ model.py               # (Optional) custom neural network
â”‚   â”œâ”€â”€ train.py                   # RLlib training script
â”‚   â””â”€â”€ run.py                     # Paper/live trading runner
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_environment.py       # Unit tests for env and data
â”œâ”€â”€ .env                          # API keys and environment variables
â”œâ”€â”€ requirements.txt              # Dependencies
â””â”€â”€ README.md
```

## ğŸš€ Setup Instructions

### 1. Clone and Install
```bash
git clone https://github.com/pyourusername/QuantTrader.git
cd QuantTrader
pip install -r requirements.txt
```

### 2. Set your Alpha Vantage API key
Create a `.env` file in the root directory:
```
ALPHA_VANTAGE_API_KEY=your_key_here
CHECKPOINT_PATH=checkpoints/PPO/checkpoint_000020/checkpoint-20
```

### 3. Fetch Historical Stock Data
```bash
python src/data/data_pipeline.py
```

### 4. Run Real-Time Streaming (optional)
```bash
# Streams live prices (prints price every 60 seconds for 5 minutes)
python src/data/data_pipeline.py --stream
```

### 5. Train the Agent
```bash
python src/train.py
```

### 6. Evaluate or Paper Trade
```bash
python src/run.py
```

## ğŸ§  Agent Algorithm

- Uses **Proximal Policy Optimization (PPO)** from Ray RLlib.
- Custom Gym environment receives state of shape `[window_size, features]` including OHLCV and technical indicators.
- Action space: **Buy, Hold, Sell**.
- Reward includes: **Portfolio return - transaction cost**.

Hyperparameters and trading environment are easily adjustable in `config/train.yaml` and `config/env.yaml`.

## ğŸ“‰ Visualization
Use the following notebook:
```bash
notebooks/results_visualization.ipynb
```
To visualize:
- Portfolio value over time
- Reward trends across episodes
- Action distribution (buy/sell/hold)

## ğŸ§ª Testing
Run unit tests using:
```bash
pytest tests/
```
## ğŸ“¦ Deployment
### Docker Support (Optional)
To containerize the project:

1. Build the Docker image:
```bash
docker build -t quanttrader .
```

2. Run a container:
```bash
docker run --env-file .env quanttrader
```

3. (Optional) Mount local data/ directory:
```bash
docker run -v $(pwd)/data:/app/data --env-file .env quanttrader
```

> Make sure to include your `.env` and downloaded data if required for the container to function correctly.

## ğŸ“„ License
MIT License

---

Made with â¤ï¸ using PyTorch + Ray RLlib
```
